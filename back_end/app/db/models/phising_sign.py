# app/db/models/phising_sign.py

from __future__ import annotations

import re
import os
import json
import hashlib
import threading
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple

import numpy as np
import torch
import torch.nn as nn

import faiss  # pip install faiss-cpu
from typing import List


# ----------------------------
# 1) AE ëª¨ë¸ êµ¬ì¡°
# ----------------------------
class PhishingFilterAE(nn.Module):
    def __init__(self, input_dim: int):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 1024), nn.ReLU(),
            nn.Linear(1024, 256), nn.ReLU(),
            nn.Linear(256, 32)
        )
        self.decoder = nn.Sequential(
            nn.Linear(32, 256), nn.ReLU(),
            nn.Linear(256, 1024), nn.ReLU(),
            nn.Linear(1024, input_dim), nn.Sigmoid()
        )

    def forward(self, x):
        return self.decoder(self.encoder(x))


# ----------------------------
# 2) ë§ˆìŠ¤í‚¹(ì „ì²˜ë¦¬)
# ----------------------------
def advanced_deidentify(text: str) -> str:
    if not isinstance(text, str):
        return ""
    titles = r"ë‹˜|ì”¨|ê³¼ì¥|íŒ€ì¥|ëŒ€ë¦¬|ë¶€ì¥|ì°¨ì¥|ì£¼ì„|ì„ ìƒë‹˜|êµìˆ˜ë‹˜"
    text = re.sub(rf'([ê°€-í£]{{2,4}})({titles})', r'[NAME]\2', text)
    text = re.sub(r'([ê°€-í£]{{2,4}})\s*(ìˆ˜ì‚¬ê´€|ê²€ì‚¬|ì‚¬ë¬´ê´€|ì¡°ì‚¬ê´€|ë“œë¦¼|ì˜¬ë¦¼)', r'[NAME] \2', text)
    text = re.sub(r'\d{2,3}-\d{3,4}-\d{4}', '[TEL]', text)
    text = re.sub(r'\d{10,14}', '[ACC]', text)
    text = re.sub(r'http[s]?://\S+', '[URL]', text)
    text = re.sub(r'\d{4,}', '[NUM]', text)
    return text


# ----------------------------
# 3) FAISS í‚¤ì›Œë“œ ìŠ¤í† ì–´
#    - TFIDF(vec)ë¡œ í‚¤ì›Œë“œ ì„ë² ë”© ë§Œë“¤ê³ 
#    - IndexIDMap2 + FlatIP(ì½”ì‚¬ì¸) ì‚¬ìš©
# ----------------------------
def _stable_id_from_text(s: str) -> int:
    """
    keyword -> stable int64 id
    (Python hashëŠ” í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ ì‚¬ìš© ê¸ˆì§€)
    """
    h = hashlib.md5(s.encode("utf-8")).hexdigest()
    # 63-bit ì–‘ìˆ˜ë¡œ ì¤„ì„
    return int(h[:16], 16) & 0x7FFFFFFFFFFFFFFF


def _l2_normalize(mat: np.ndarray, eps: float = 1e-12) -> np.ndarray:
    norms = np.linalg.norm(mat, axis=1, keepdims=True)
    return mat / (norms + eps)


class FaissKeywordStore:
    def __init__(
        self,
        vec,  # TfidfVectorizer
        dim: int,
        index_path: str | Path,
        meta_path: str | Path,
    ):
        self.vec = vec
        self.dim = int(dim)
        self.index_path = Path(index_path)
        self.meta_path = Path(meta_path)

        self._lock = threading.RLock()

        # id <-> keyword
        self.id_to_kw: Dict[int, str] = {}
        self.kw_to_id: Dict[str, int] = {}

        self.index = self._create_empty_index()
        self.load()

    def _create_empty_index(self) -> faiss.Index:
        base = faiss.IndexFlatIP(self.dim)  # cosine similarity = normalized vec + inner product
        return faiss.IndexIDMap2(base)

    def _embed_keywords(self, keywords: List[str]) -> np.ndarray:
        # TF-IDFëŠ” sparse -> dense float32
        cleaned = [advanced_deidentify(k) for k in keywords]
        x = self.vec.transform(cleaned).toarray().astype("float32")
        x = _l2_normalize(x)
        return x

    def _embed_sentence(self, sentence: str) -> np.ndarray:
        cleaned = advanced_deidentify(sentence)
        x = self.vec.transform([cleaned]).toarray().astype("float32")
        x = _l2_normalize(x)
        return x

    def load(self) -> None:
        with self._lock:
            if self.index_path.exists():
                self.index = faiss.read_index(str(self.index_path))

            if self.meta_path.exists():
                data = json.loads(self.meta_path.read_text(encoding="utf-8"))
                # jsonì€ keyê°€ strë¡œ ì €ì¥ë˜ë¯€ë¡œ intë¡œ ë³€í™˜
                self.id_to_kw = {int(k): v for k, v in data.get("id_to_kw", {}).items()}
                self.kw_to_id = {k: int(v) for k, v in data.get("kw_to_id", {}).items()}

    def save(self) -> None:
        with self._lock:
            self.index_path.parent.mkdir(parents=True, exist_ok=True)
            faiss.write_index(self.index, str(self.index_path))
            payload = {
                "id_to_kw": {str(k): v for k, v in self.id_to_kw.items()},
                "kw_to_id": self.kw_to_id,
            }
            self.meta_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

    def rebuild(self, keywords: List[str]) -> int:
        """
        ì „ì²´ ì¬ì ì¬(ë¦¬ë¹Œë“œ): ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ êµì²´
        """
        keywords = [k.strip() for k in keywords if isinstance(k, str) and k.strip()]
        if not keywords:
            with self._lock:
                self.index = self._create_empty_index()
                self.id_to_kw.clear()
                self.kw_to_id.clear()
                self.save()
            return 0

        with self._lock:
            self.index = self._create_empty_index()
            self.id_to_kw.clear()
            self.kw_to_id.clear()

            ids = np.array([_stable_id_from_text(k) for k in keywords], dtype="int64")
            vecs = self._embed_keywords(keywords)

            self.index.add_with_ids(vecs, ids)

            for kw, _id in zip(keywords, ids.tolist()):
                self.id_to_kw[_id] = kw
                self.kw_to_id[kw] = _id

            self.save()
            return len(keywords)

    def upsert(self, keywords: List[str]) -> Dict[str, Any]:
        """
        í‚¤ì›Œë“œ ì¶”ê°€/ì—…ë°ì´íŠ¸:
        - ì´ë¯¸ ì¡´ì¬í•˜ë©´ remove_ids í›„ add_with_idsë¡œ êµì²´
        """
        keywords = [k.strip() for k in keywords if isinstance(k, str) and k.strip()]
        if not keywords:
            return {"added": 0, "updated": 0, "total": self.index.ntotal}

        with self._lock:
            add_list: List[str] = []
            add_ids: List[int] = []
            updated = 0
            added = 0

            # ë¨¼ì € remove í•  idë“¤ ìˆ˜ì§‘
            to_remove: List[int] = []
            for kw in keywords:
                _id = _stable_id_from_text(kw)
                if _id in self.id_to_kw:
                    to_remove.append(_id)
                    updated += 1
                else:
                    added += 1
                add_list.append(kw)
                add_ids.append(_id)

            if to_remove:
                selector = faiss.IDSelectorBatch(np.array(to_remove, dtype="int64"))
                self.index.remove_ids(selector)

            vecs = self._embed_keywords(add_list)
            ids = np.array(add_ids, dtype="int64")
            self.index.add_with_ids(vecs, ids)

            for kw, _id in zip(add_list, add_ids):
                self.id_to_kw[_id] = kw
                self.kw_to_id[kw] = _id

            self.save()
            return {"added": added, "updated": updated, "total": self.index.ntotal}

    def remove(self, keywords: List[str]) -> int:
        keywords = [k.strip() for k in keywords if isinstance(k, str) and k.strip()]
        if not keywords:
            return 0

        with self._lock:
            ids = []
            for kw in keywords:
                _id = _stable_id_from_text(kw)
                if _id in self.id_to_kw:
                    ids.append(_id)

            if not ids:
                return 0

            selector = faiss.IDSelectorBatch(np.array(ids, dtype="int64"))
            removed = self.index.remove_ids(selector)

            for _id in ids:
                kw = self.id_to_kw.pop(_id, None)
                if kw:
                    self.kw_to_id.pop(kw, None)

            self.save()
            return int(removed)

    def search(self, sentence: str, topk: int = 10, min_sim: float = 0.25) -> List[Dict[str, Any]]:
        """
        sentence -> topK í‚¤ì›Œë“œ (cosine similarity)
        """
        if self.index.ntotal == 0:
            return []

        x = self._embed_sentence(sentence)
        with self._lock:
            sims, ids = self.index.search(x, topk)

        out = []
        for sim, _id in zip(sims[0].tolist(), ids[0].tolist()):
            if _id == -1:
                continue
            if sim < min_sim:
                continue
            kw = self.id_to_kw.get(int(_id))
            if not kw:
                continue
            out.append({"keyword": kw, "sim": float(sim), "id": int(_id)})
        return out


# ----------------------------
# 4) AE íƒì§€ê¸° + (FAISS í‚¤ì›Œë“œ)
# ----------------------------
class PhishingDetectorAE:
    """
    - ckpt í¬ë§·:
      {'vec': TfidfVectorizer, 'input_dim': int, 'state': model_state_dict}
    - keywords íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ FAISSì—ì„œ ìë™ ê²€ìƒ‰
    """

    def __init__(self, model_path: str | Path, kw_store: Optional[FaissKeywordStore] = None):
        self.model_path = str(model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        try:
            ckpt = torch.load(self.model_path, map_location=self.device, weights_only=False)
        except TypeError:
            ckpt = torch.load(self.model_path, map_location=self.device)

        self.vec = ckpt["vec"]
        self.input_dim = int(ckpt["input_dim"])
        self.model = PhishingFilterAE(self.input_dim).to(self.device)
        self.model.load_state_dict(ckpt["state"])
        self.model.eval()

        self.kw_store = kw_store  # ë‚˜ì¤‘ì— attach ê°€ëŠ¥

    def attach_kw_store(self, kw_store: FaissKeywordStore) -> None:
        self.kw_store = kw_store

    def _score(self, sentence: str, keywords: Optional[List[str]] = None) -> Dict[str, Any]:
        cleaned = advanced_deidentify(sentence)
        x_vec = self.vec.transform([cleaned]).toarray()
        x_tensor = torch.FloatTensor(x_vec).to(self.device)

        with torch.no_grad():
            pred = self.model(x_tensor)
            base_loss = torch.abs(pred - x_tensor).sum().item()

        detected_kw = keywords or []
        penalty = 1.0
        for _ in detected_kw:
            penalty *= 20.0

        final_score = base_loss * penalty

        if final_score > 150:
            label = "ğŸš¨ ì°¨ë‹¨"
        elif final_score > 80:
            label = "âš ï¸ ì£¼ì˜"
        else:
            label = "âœ… ì •ìƒ"

        return {
            "result": label,
            "score": round(float(final_score), 2),
            "base_loss": round(float(base_loss), 4),
            "keywords": detected_kw,
            "cleaned": cleaned,
        }

    def predict(
        self,
        sentence: str,
        keywords: Optional[List[str]] = None,
        *,
        faiss_topk: int = 10,
        faiss_min_sim: float = 0.25,
    ) -> Dict[str, Any]:
        if not sentence or not isinstance(sentence, str):
            return {"result": "âœ… ì •ìƒ", "score": 0.0, "keywords": []}

        # âœ… keywordsê°€ ì•ˆ ë“¤ì–´ì˜¤ë©´ FAISSì—ì„œ ìë™ìœ¼ë¡œ ë½‘ì•„ ì‚¬ìš©
        faiss_hits: List[Dict[str, Any]] = []
        if (keywords is None) and self.kw_store is not None:
            faiss_hits = self.kw_store.search(sentence, topk=faiss_topk, min_sim=faiss_min_sim)
            keywords = [h["keyword"] for h in faiss_hits]

        out = self._score(sentence, keywords=keywords)
        out["faiss_hits"] = faiss_hits  # ë””ë²„ê¹…/ì„¤ëª…ìš©(ì›ì¹˜ ì•Šìœ¼ë©´ ì œê±°)
        return out


# ----------------------------
# 5) ì‹±ê¸€í†¤ ì´ˆê¸°í™”
# ----------------------------
DEFAULT_AE_PATH = Path("assets/models/final_ae.pth")

# FAISS ì €ì¥ ê²½ë¡œ
DEFAULT_FAISS_INDEX = Path("assets/faiss/keyword.index")
DEFAULT_FAISS_META = Path("assets/faiss/keyword_meta.json")

# ë¨¼ì € AE ë¡œë“œ
ae_detector = PhishingDetectorAE(DEFAULT_AE_PATH)

# AEì˜ vec/input_dimìœ¼ë¡œ FAISS store ìƒì„±/ë¡œë“œ
kw_store = FaissKeywordStore(
    vec=ae_detector.vec,
    dim=ae_detector.input_dim,
    index_path=DEFAULT_FAISS_INDEX,
    meta_path=DEFAULT_FAISS_META,
)

# AEì— attach
ae_detector.attach_kw_store(kw_store)


def get_keywords_from_faiss(sentence: str, topk: int = 5, min_sim: float = 0.35) -> List[str]:
    """
    sentenceë¥¼ FAISSì—ì„œ ê²€ìƒ‰í•´ ê´€ë ¨ í‚¤ì›Œë“œ topKë¥¼ ë°˜í™˜
    """
    hits = kw_store.search(sentence, topk=topk, min_sim=min_sim)
    return [h["keyword"] for h in hits]
